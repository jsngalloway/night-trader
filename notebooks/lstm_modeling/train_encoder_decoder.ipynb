{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "H1wRd8l8xpGV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, optimizers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "register_matplotlib_converters()\n",
    "sns.set(style=\"darkgrid\", font_scale=1.5)\n",
    "\n",
    "LENGTH = 600\n",
    "LOOK_AHEAD_LENGTH = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "tf.config.experimental.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wqVeZx_0x5AA",
    "outputId": "dd26d715-c763-4693-c383-a107a2e8b1ad"
   },
   "outputs": [],
   "source": [
    "# model = models.load_model('../../models/12kepochs500back40forward_encodedecode')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "el09LBBxxpGa"
   },
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "xNnzQCbwxpGb"
   },
   "outputs": [],
   "source": [
    "def shapeData(data, length, look_ahead_length):\n",
    "    hist = []\n",
    "    target = []\n",
    "\n",
    "    for i in range(len(data)-length-look_ahead_length):\n",
    "        x = data[i:i+length]\n",
    "        y = data[i+length:i+length+look_ahead_length]\n",
    "        hist.append(x)\n",
    "        target.append(y)\n",
    "    \n",
    "    # Convert into numpy arrays and shape correctly (len(dataset), length) and (len(dataset), 1) respectivly\n",
    "    hist = np.array(hist)\n",
    "    target = np.array(target)\n",
    "    hist = hist.reshape((len(hist), length, 1))\n",
    "\n",
    "    return(hist, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "eNsZw9tbxpGb"
   },
   "outputs": [],
   "source": [
    "def trainModel(datasets, length, model=None, quiet=False):\n",
    "    for dataset in datasets:\n",
    "        X_train, y_train = shapeData(dataset, length, LOOK_AHEAD_LENGTH)\n",
    "\n",
    "        if not model:\n",
    "            # Create model and compile if not already passed a model\n",
    "            model = tf.keras.Sequential()\n",
    "            model.add(layers.LSTM(units=20, input_shape=(length,1), dropout=0.05, activation='tanh'))\n",
    "            model.add(layers.RepeatVector(LOOK_AHEAD_LENGTH))\n",
    "            model.add(layers.LSTM(units=60, return_sequences=True, dropout=0.05, activation='tanh'))\n",
    "            model.add(layers.TimeDistributed(layers.Dense(20)))\n",
    "            model.add(layers.TimeDistributed(layers.Dense(1)))\n",
    "            opt = optimizers.Adam(learning_rate=0.0001)\n",
    "            model.compile(optimizer=opt, loss='mean_squared_error')\n",
    "            print(model.summary())\n",
    "        else:\n",
    "            print(\"Continuing training from where we left off...\")\n",
    "\n",
    "        # Perform training\n",
    "        output = 1\n",
    "        if quiet:\n",
    "            output = 0\n",
    "        history = model.fit(X_train, y_train, epochs=1000, batch_size=64, verbose=output, shuffle=False)\n",
    "\n",
    "        # Show loss\n",
    "        if not quiet:\n",
    "            loss = history.history['loss']\n",
    "            epoch_count = range(1, len(loss) + 1)\n",
    "            plt.figure(figsize=(6,4))\n",
    "            plt.plot(epoch_count, loss, 'r--')\n",
    "            plt.legend(['Training Loss'])\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.show()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "_YoJcI5nxpGc"
   },
   "outputs": [],
   "source": [
    "def scaleData(paths):\n",
    "    scaler = MinMaxScaler()\n",
    "    datasets = []\n",
    "    for path in paths:\n",
    "        # perform partial fits on all datasets\n",
    "        \n",
    "        new_df = pd.DataFrame()\n",
    "        new_df[\"price\"] = pd.read_csv(path)[[\"high_price\",\"low_price\"]].mean(axis=1)\n",
    "        multiple = len(new_df) - (len(new_df) % 64)\n",
    "        new_df = new_df.tail(multiple)\n",
    "        \n",
    "        datasets.append(new_df)\n",
    "\n",
    "        scaler = scaler.partial_fit(datasets[-1])\n",
    "    for i in range(len(datasets)):\n",
    "        # once all partial fits have been performed, transform every file\n",
    "        datasets[i] = scaler.transform(datasets[i])\n",
    "    return (datasets, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "dDI5LkDFxpGd",
    "outputId": "4a210002-70fd-4815-8bd4-0ec5093df405",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 20)                1760      \n",
      "_________________________________________________________________\n",
      "repeat_vector (RepeatVector) (None, 40, 20)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 40, 60)            19440     \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 40, 20)            1220      \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 40, 1)             21        \n",
      "=================================================================\n",
      "Total params: 22,441\n",
      "Trainable params: 22,441\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 0.0038\n",
      "Epoch 2/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 0.0027\n",
      "Epoch 3/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 0.0025\n",
      "Epoch 4/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 0.0021\n",
      "Epoch 5/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 0.0018\n",
      "Epoch 6/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 0.0016\n",
      "Epoch 7/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 0.0014\n",
      "Epoch 8/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 0.0013\n",
      "Epoch 9/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 0.0012\n",
      "Epoch 10/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 0.0011\n",
      "Epoch 11/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 0.0011\n",
      "Epoch 12/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 0.0011\n",
      "Epoch 13/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 0.0010\n",
      "Epoch 14/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 0.0010\n",
      "Epoch 15/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 0.0010\n",
      "Epoch 16/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 0.0010\n",
      "Epoch 17/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 9.9651e-04\n",
      "Epoch 18/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 9.9203e-04\n",
      "Epoch 19/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 9.9059e-04\n",
      "Epoch 20/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 9.7937e-04\n",
      "Epoch 21/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 9.7614e-04\n",
      "Epoch 22/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 9.7024e-04\n",
      "Epoch 23/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 9.6722e-04\n",
      "Epoch 24/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 9.6180e-04\n",
      "Epoch 25/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 9.5678e-04\n",
      "Epoch 26/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 9.4591e-04\n",
      "Epoch 27/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 9.4989e-04\n",
      "Epoch 28/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 9.5420e-04\n",
      "Epoch 29/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 9.4943e-04\n",
      "Epoch 30/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 9.4251e-04\n",
      "Epoch 31/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 9.3823e-04\n",
      "Epoch 32/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 9.2247e-04\n",
      "Epoch 33/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 9.3108e-04\n",
      "Epoch 34/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 9.2937e-04\n",
      "Epoch 35/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 9.1901e-04\n",
      "Epoch 36/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 9.0913e-04\n",
      "Epoch 37/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 9.0661e-04\n",
      "Epoch 38/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 9.0956e-04\n",
      "Epoch 39/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 9.0970e-04\n",
      "Epoch 40/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 9.0374e-04\n",
      "Epoch 41/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 8.9755e-04\n",
      "Epoch 42/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 8.9338e-04\n",
      "Epoch 43/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 8.8720e-04\n",
      "Epoch 44/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 8.8900e-04\n",
      "Epoch 45/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 8.8544e-04\n",
      "Epoch 46/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 8.7399e-04\n",
      "Epoch 47/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 8.6279e-04\n",
      "Epoch 48/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 8.6224e-04\n",
      "Epoch 49/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 8.5040e-04\n",
      "Epoch 50/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 8.5880e-04\n",
      "Epoch 51/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 8.3855e-04\n",
      "Epoch 52/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 8.3059e-04\n",
      "Epoch 53/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 8.3153e-04\n",
      "Epoch 54/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 8.2560e-04\n",
      "Epoch 55/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 8.1895e-04\n",
      "Epoch 56/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 8.2423e-04\n",
      "Epoch 57/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 8.0490e-04\n",
      "Epoch 58/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 8.1996e-04\n",
      "Epoch 59/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 8.0905e-04\n",
      "Epoch 60/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 8.0132e-04\n",
      "Epoch 61/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 7.9505e-04\n",
      "Epoch 62/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 7.9601e-04\n",
      "Epoch 63/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 7.8609e-04\n",
      "Epoch 64/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 7.8521e-04\n",
      "Epoch 65/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 7.7923e-04\n",
      "Epoch 66/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 7.8419e-04\n",
      "Epoch 67/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 7.6695e-04\n",
      "Epoch 68/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 7.6715e-04\n",
      "Epoch 69/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 7.7154e-04\n",
      "Epoch 70/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 7.5162e-04\n",
      "Epoch 71/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 7.4953e-04\n",
      "Epoch 72/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 7.4937e-04\n",
      "Epoch 73/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 7.4062e-04\n",
      "Epoch 74/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 7.3539e-04\n",
      "Epoch 75/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 7.3502e-04\n",
      "Epoch 76/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 7.3228e-04\n",
      "Epoch 77/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 7.3022e-04\n",
      "Epoch 78/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 7.2430e-04\n",
      "Epoch 79/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 7.2119e-04\n",
      "Epoch 80/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 7.2032e-04\n",
      "Epoch 81/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "961/961 [==============================] - 27s 28ms/step - loss: 7.1858e-04\n",
      "Epoch 82/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 7.0456e-04\n",
      "Epoch 83/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 7.0157e-04\n",
      "Epoch 84/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 7.0387e-04\n",
      "Epoch 85/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 6.9937e-04\n",
      "Epoch 86/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 7.0346e-04\n",
      "Epoch 87/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 6.9419e-04\n",
      "Epoch 88/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 6.9733e-04\n",
      "Epoch 89/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 6.8833e-04\n",
      "Epoch 90/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 6.8116e-04\n",
      "Epoch 91/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 6.8853e-04\n",
      "Epoch 92/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 6.8665e-04\n",
      "Epoch 93/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 6.7845e-04\n",
      "Epoch 94/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 6.8040e-04\n",
      "Epoch 95/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 6.7945e-04\n",
      "Epoch 96/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 6.7181e-04\n",
      "Epoch 97/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 6.7451e-04\n",
      "Epoch 98/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 6.7502e-04\n",
      "Epoch 99/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 6.6845e-04\n",
      "Epoch 100/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 6.6661e-04\n",
      "Epoch 101/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 6.6820e-04\n",
      "Epoch 102/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 6.6092e-04\n",
      "Epoch 103/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 6.6488e-04\n",
      "Epoch 104/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 6.6540e-04\n",
      "Epoch 105/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 6.6203e-04\n",
      "Epoch 106/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 6.5573e-04\n",
      "Epoch 107/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 6.5303e-04\n",
      "Epoch 108/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 6.5161e-04\n",
      "Epoch 109/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 6.4759e-04\n",
      "Epoch 110/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 6.5111e-04\n",
      "Epoch 111/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 6.4563e-04\n",
      "Epoch 112/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 6.3865e-04\n",
      "Epoch 113/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 6.4581e-04\n",
      "Epoch 114/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 6.4369e-04\n",
      "Epoch 115/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 6.3768e-04\n",
      "Epoch 116/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 6.3442e-04\n",
      "Epoch 117/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 6.3022e-04\n",
      "Epoch 118/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 6.3238e-04\n",
      "Epoch 119/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 6.2129e-04\n",
      "Epoch 120/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 6.2372e-04\n",
      "Epoch 121/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 6.1593e-04\n",
      "Epoch 122/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 6.1430e-04\n",
      "Epoch 123/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 6.1048e-04\n",
      "Epoch 124/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 6.0163e-04\n",
      "Epoch 125/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 5.9741e-04\n",
      "Epoch 126/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 5.9659e-04\n",
      "Epoch 127/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 5.9995e-04\n",
      "Epoch 128/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 5.8929e-04\n",
      "Epoch 129/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 5.8211e-04\n",
      "Epoch 130/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 5.7828e-04\n",
      "Epoch 131/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 5.7761e-04\n",
      "Epoch 132/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 5.6786e-04\n",
      "Epoch 133/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 5.6286e-04\n",
      "Epoch 134/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 5.6011e-04\n",
      "Epoch 135/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 5.4901e-04\n",
      "Epoch 136/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 5.4397e-04\n",
      "Epoch 137/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 5.4273e-04\n",
      "Epoch 138/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 5.3073e-04\n",
      "Epoch 139/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 5.1680e-04\n",
      "Epoch 140/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 5.2169e-04\n",
      "Epoch 141/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 5.1553e-04\n",
      "Epoch 142/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 5.0434e-04\n",
      "Epoch 143/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 4.9795e-04\n",
      "Epoch 144/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 4.9556e-04\n",
      "Epoch 145/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 4.9096e-04\n",
      "Epoch 146/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 4.7221e-04\n",
      "Epoch 147/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 4.7459e-04\n",
      "Epoch 148/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 4.6771e-04\n",
      "Epoch 149/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 4.7167e-04\n",
      "Epoch 150/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 4.5221e-04\n",
      "Epoch 151/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 4.5004e-04\n",
      "Epoch 152/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 4.5859e-04\n",
      "Epoch 153/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 4.5423e-04\n",
      "Epoch 154/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 4.4993e-04\n",
      "Epoch 155/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 4.4057e-04\n",
      "Epoch 156/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 4.3349e-04\n",
      "Epoch 157/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 4.3274e-04\n",
      "Epoch 158/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 4.3012e-04\n",
      "Epoch 159/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 4.3260e-04\n",
      "Epoch 160/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 4.2975e-04\n",
      "Epoch 161/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 4.2797e-04\n",
      "Epoch 162/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 4.2449e-04\n",
      "Epoch 163/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 4.1605e-04\n",
      "Epoch 164/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 4.1675e-04\n",
      "Epoch 165/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 4.0834e-04 0s\n",
      "Epoch 166/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 4.1074e-04\n",
      "Epoch 167/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 4.0773e-04\n",
      "Epoch 168/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 4.0065e-04\n",
      "Epoch 169/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 3.9629e-04\n",
      "Epoch 170/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 4.0172e-04\n",
      "Epoch 171/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "961/961 [==============================] - 27s 28ms/step - loss: 3.8940e-04\n",
      "Epoch 172/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 3.9645e-04\n",
      "Epoch 173/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 3.8300e-04\n",
      "Epoch 174/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 3.7991e-04\n",
      "Epoch 175/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 3.7594e-04\n",
      "Epoch 176/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 3.7520e-04\n",
      "Epoch 177/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 3.7669e-04\n",
      "Epoch 178/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 3.7469e-04\n",
      "Epoch 179/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 3.6274e-04\n",
      "Epoch 180/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 3.6442e-04\n",
      "Epoch 181/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 3.6533e-04\n",
      "Epoch 182/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 3.6767e-04\n",
      "Epoch 183/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 3.5922e-04\n",
      "Epoch 184/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 3.5590e-04\n",
      "Epoch 185/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 3.5210e-04\n",
      "Epoch 186/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 3.5128e-04\n",
      "Epoch 187/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 3.5108e-04\n",
      "Epoch 188/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 3.5003e-04\n",
      "Epoch 189/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 3.4613e-04\n",
      "Epoch 190/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 3.3939e-04\n",
      "Epoch 191/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 3.3653e-04\n",
      "Epoch 192/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 3.3262e-04\n",
      "Epoch 193/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 3.3480e-04\n",
      "Epoch 194/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 3.3257e-04\n",
      "Epoch 195/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 3.2811e-04\n",
      "Epoch 196/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 3.2962e-04 0s - loss: 3.267\n",
      "Epoch 197/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 3.2773e-04\n",
      "Epoch 198/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 3.2311e-04\n",
      "Epoch 199/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 3.2745e-04\n",
      "Epoch 200/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 3.2108e-04\n",
      "Epoch 201/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 3.2251e-04\n",
      "Epoch 202/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 3.1667e-04\n",
      "Epoch 203/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 3.1997e-04\n",
      "Epoch 204/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 3.1595e-04\n",
      "Epoch 205/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 3.1973e-04\n",
      "Epoch 206/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 3.1174e-04\n",
      "Epoch 207/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 3.1066e-04\n",
      "Epoch 208/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 3.0789e-04\n",
      "Epoch 209/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 3.0735e-04\n",
      "Epoch 210/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 3.0798e-04\n",
      "Epoch 211/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 3.0578e-04\n",
      "Epoch 212/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 3.0212e-04\n",
      "Epoch 213/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 2.9968e-04\n",
      "Epoch 214/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 3.0011e-04\n",
      "Epoch 215/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 3.0570e-04\n",
      "Epoch 216/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 2.9263e-04\n",
      "Epoch 217/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 2.9590e-04\n",
      "Epoch 218/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 2.8849e-04\n",
      "Epoch 219/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 2.9017e-04\n",
      "Epoch 220/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 2.9599e-04\n",
      "Epoch 221/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 2.8445e-04\n",
      "Epoch 222/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 2.8389e-04\n",
      "Epoch 223/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 2.8710e-04\n",
      "Epoch 224/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 2.8478e-04\n",
      "Epoch 225/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 2.8304e-04\n",
      "Epoch 226/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 2.7786e-04\n",
      "Epoch 227/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 2.7870e-04\n",
      "Epoch 228/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 2.8094e-04\n",
      "Epoch 229/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 2.8273e-04\n",
      "Epoch 230/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 2.7383e-04\n",
      "Epoch 231/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 2.7756e-04\n",
      "Epoch 232/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 2.7320e-04\n",
      "Epoch 233/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 2.7283e-04\n",
      "Epoch 234/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 2.7323e-04\n",
      "Epoch 235/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 2.7469e-04\n",
      "Epoch 236/1000\n",
      "961/961 [==============================] - 27s 28ms/step - loss: 2.7148e-04\n",
      "Epoch 237/1000\n",
      "280/961 [=======>......................] - ETA: 19s - loss: 3.0200e-04"
     ]
    }
   ],
   "source": [
    "model = None\n",
    "for i in np.arange(1, 20):\n",
    "    paths = [\"../../data/ETH.csv\"]\n",
    "    datasets, scaler = scaleData(paths)\n",
    "    model = trainModel(datasets, LENGTH, model)\n",
    "    model.save(f\"E-D-{i}k-600back-40forward-sm\")\n",
    "    print(f\"Completed {i}k epochs and saved model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZlhYBDTyxpGf"
   },
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s4GVUDzOxpGg"
   },
   "outputs": [],
   "source": [
    "def testModel(model, path_to_testing_dataset, quiet=False):\n",
    "\n",
    "    datasets, scaler = scaleData([path_to_testing_dataset])\n",
    "\n",
    "    hist, actual = shapeData(datasets[0], LENGTH, LOOK_AHEAD_LENGTH)\n",
    "\n",
    "    pred = model.predict(hist)\n",
    "\n",
    "    \n",
    "    # for p in pred:\n",
    "    # pred_transformed.append(scaler.inverse_transform(p))\n",
    "    pred_transformed = scaler.inverse_transform(pred[0])\n",
    "    actual_transformed = scaler.inverse_transform(actual[0])\n",
    "    hist_transformed = scaler.inverse_transform(hist[0])\n",
    "\n",
    "    \n",
    "    # print(pred[500])\n",
    "    # print(hist[0])\n",
    "    if not quiet:\n",
    "        plt.figure(figsize=(12,8))\n",
    "        plt.plot(hist_transformed, color='blue', label='History')\n",
    "        plt.plot(np.arange(len(hist_transformed)-1,len(hist_transformed)+LOOK_AHEAD_LENGTH-1),pred_transformed, color='red', label='Prediction')\n",
    "        plt.plot(np.arange(len(hist_transformed)-1,len(hist_transformed)+LOOK_AHEAD_LENGTH-1),actual_transformed, color='purple', label='Actual')\n",
    "        plt.title('ETH Price Prediction')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        pred_transformed = scaler.inverse_transform(pred[900])\n",
    "        actual_transformed = scaler.inverse_transform(actual[900])\n",
    "        hist_transformed = scaler.inverse_transform(hist[900])\n",
    "        plt.figure(figsize=(12,8))\n",
    "        plt.plot(hist_transformed, color='blue', label='History')\n",
    "        plt.plot(np.arange(len(hist_transformed)-1,len(hist_transformed)+LOOK_AHEAD_LENGTH-1),pred_transformed, color='red', label='Prediction')\n",
    "        plt.plot(np.arange(len(hist_transformed)-1,len(hist_transformed)+LOOK_AHEAD_LENGTH-1),actual_transformed, color='purple', label='Actual')\n",
    "        plt.title('ETH Price Prediction')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "a1JG-d5fxpGg",
    "outputId": "7dde16a1-1953-46c4-d9a2-a2c61bb8dddb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "testModel(model, \"../../data/ETH.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "33Q-fXVRxpGh"
   },
   "source": [
    "# Single Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "m7frpJ-fxpGh",
    "outputId": "1331ec09-3d56-45ee-cc9c-ace97934287d"
   },
   "outputs": [],
   "source": [
    "# For example, if we just want to predict the next timestep in the dataset we can prepare it as such:\n",
    "\n",
    "# 1. get the [length] last points from the data set since that's what we care about\n",
    "length = LENGTH\n",
    "whole_df = pd.read_csv('../../data/ETH.csv')\n",
    "# most_recent_period = np.array(recent_history)\n",
    "recent_history = pd.DataFrame({'price': whole_df[[\"high_price\",\"low_price\"]].mean(axis=1)}).tail(length)\n",
    "recent_history = recent_history.set_index(pd.DatetimeIndex(pd.to_datetime(whole_df['begins_at'].tail(length))))\n",
    "\n",
    "# 2. convert to numpy array \n",
    "most_recent_period = np.array(recent_history)\n",
    "\n",
    "# 3. normalize data\n",
    "scaler = MinMaxScaler()\n",
    "most_recent_period_scaled = scaler.fit_transform(most_recent_period)\n",
    "\n",
    "# 4. reshape to the 3D tensor we expected (1, length, 1)\n",
    "most_recent_period_scaled_shaped = most_recent_period_scaled.reshape((1, length, 1))\n",
    "\n",
    "# 5. Predict\n",
    "prediction = model.predict(most_recent_period_scaled_shaped)\n",
    "# print(prediction[0])\n",
    "# 6. Un-normalize the data\n",
    "result = scaler.inverse_transform(prediction[0])\n",
    "recent_history_unscaled = scaler.inverse_transform(most_recent_period_scaled)\n",
    "recent_history.price = recent_history_unscaled\n",
    "\n",
    "# print(result)\n",
    "pred_df = pd.DataFrame(result)\n",
    "# pred_df = pred_df.set_index(pd.DatetimeIndex(pd.date_range(recent_history.index[-1], periods=len(result[0]), freq=\"15s\")))\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.plot(recent_history_unscaled)\n",
    "plt.plot(np.arange(length, length+LOOK_AHEAD_LENGTH),result)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "encode_decoder.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
